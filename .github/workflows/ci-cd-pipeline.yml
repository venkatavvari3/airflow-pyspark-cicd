name: PySpark Airflow CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy to'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - prod

env:
  PYTHON_VERSION: '3.9'
  SPARK_VERSION: '3.4.0'
  HADOOP_VERSION: '3.3'

jobs:
  lint-and-test:
    name: Lint and Test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        
    - name: Lint with flake8
      run: |
        flake8 src/ dags/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/ dags/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        
    - name: Lint with pylint
      run: |
        pylint src/ dags/ --exit-zero
        
    - name: Type checking with mypy
      run: |
        mypy src/ dags/ --ignore-missing-imports
        
    - name: Run unit tests
      run: |
        python -m pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html
        
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  pyspark-tests:
    name: PySpark Integration Tests
    runs-on: ubuntu-latest
    needs: lint-and-test
    
    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: airflow_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Set up Java
      uses: actions/setup-java@v3
      with:
        distribution: 'temurin'
        java-version: '11'
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        
    - name: Run PySpark integration tests
      env:
        SPARK_LOCAL_IP: 127.0.0.1
      run: |
        python -m pytest tests/integration/ -v --tb=short
        
    - name: Run Airflow DAG tests
      env:
        AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://postgres:postgres@localhost/airflow_test
        AIRFLOW__CORE__EXECUTOR: SequentialExecutor
      run: |
        airflow db init
        python -m pytest tests/dags/ -v

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: lint-and-test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
        
    - name: Run safety check
      run: |
        safety check -r requirements.txt
        
    - name: Run bandit security scan
      run: |
        bandit -r src/ dags/ -f json -o bandit-report.json
        
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: bandit-report.json

  build-docker:
    name: Build Docker Images
    runs-on: ubuntu-latest
    needs: [lint-and-test, pyspark-tests]
    if: github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ secrets.CONTAINER_REGISTRY }}
        username: ${{ secrets.REGISTRY_USERNAME }}
        password: ${{ secrets.REGISTRY_PASSWORD }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ secrets.CONTAINER_REGISTRY }}/pyspark-airflow
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
          
    - name: Build and push PySpark image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile.pyspark
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        
    - name: Build and push Airflow image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: ./docker/Dockerfile.airflow
        push: true
        tags: ${{ secrets.CONTAINER_REGISTRY }}/airflow-custom:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-dev:
    name: Deploy to Development
    runs-on: ubuntu-latest
    needs: [build-docker, security-scan]
    if: github.ref == 'refs/heads/develop'
    environment: development
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}
        
    - name: Deploy to EKS
      env:
        CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME_DEV }}
        NAMESPACE: airflow-dev
      run: |
        aws eks update-kubeconfig --name $CLUSTER_NAME
        kubectl set image deployment/airflow-scheduler airflow=${{ secrets.CONTAINER_REGISTRY }}/airflow-custom:${{ github.sha }} -n $NAMESPACE
        kubectl set image deployment/airflow-webserver airflow=${{ secrets.CONTAINER_REGISTRY }}/airflow-custom:${{ github.sha }} -n $NAMESPACE
        kubectl rollout status deployment/airflow-scheduler -n $NAMESPACE
        kubectl rollout status deployment/airflow-webserver -n $NAMESPACE
        
    - name: Run smoke tests
      run: |
        python -m pytest tests/smoke/ -v --env=dev

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [build-docker, security-scan]
    if: github.ref == 'refs/heads/main'
    environment: staging
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}
        
    - name: Deploy to EKS
      env:
        CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME_STAGING }}
        NAMESPACE: airflow-staging
      run: |
        aws eks update-kubeconfig --name $CLUSTER_NAME
        kubectl set image deployment/airflow-scheduler airflow=${{ secrets.CONTAINER_REGISTRY }}/airflow-custom:${{ github.sha }} -n $NAMESPACE
        kubectl set image deployment/airflow-webserver airflow=${{ secrets.CONTAINER_REGISTRY }}/airflow-custom:${{ github.sha }} -n $NAMESPACE
        kubectl rollout status deployment/airflow-scheduler -n $NAMESPACE
        kubectl rollout status deployment/airflow-webserver -n $NAMESPACE
        
    - name: Run integration tests
      run: |
        python -m pytest tests/integration/ -v --env=staging

  deploy-production:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: deploy-staging
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Manual approval required
      uses: trstringer/manual-approval@v1
      with:
        secret: ${{ github.TOKEN }}
        approvers: ${{ secrets.PROD_APPROVERS }}
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID_PROD }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY_PROD }}
        aws-region: ${{ secrets.AWS_REGION }}
        
    - name: Deploy to Production EKS
      env:
        CLUSTER_NAME: ${{ secrets.EKS_CLUSTER_NAME_PROD }}
        NAMESPACE: airflow-prod
      run: |
        aws eks update-kubeconfig --name $CLUSTER_NAME
        kubectl set image deployment/airflow-scheduler airflow=${{ secrets.CONTAINER_REGISTRY }}/airflow-custom:${{ github.sha }} -n $NAMESPACE
        kubectl set image deployment/airflow-webserver airflow=${{ secrets.CONTAINER_REGISTRY }}/airflow-custom:${{ github.sha }} -n $NAMESPACE
        kubectl rollout status deployment/airflow-scheduler -n $NAMESPACE
        kubectl rollout status deployment/airflow-webserver -n $NAMESPACE
        
    - name: Run production smoke tests
      run: |
        python -m pytest tests/smoke/ -v --env=prod
        
    - name: Notify deployment success
      if: success()
      run: |
        echo "Production deployment successful for commit ${{ github.sha }}"
        
    - name: Rollback on failure
      if: failure()
      run: |
        kubectl rollout undo deployment/airflow-scheduler -n $NAMESPACE
        kubectl rollout undo deployment/airflow-webserver -n $NAMESPACE

  cleanup:
    name: Cleanup
    runs-on: ubuntu-latest
    needs: [deploy-dev, deploy-staging, deploy-production]
    if: always()
    
    steps:
    - name: Clean up old images
      run: |
        echo "Cleaning up old container images..."
        # Add cleanup logic here
